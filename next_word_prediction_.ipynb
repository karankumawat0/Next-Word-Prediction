{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh1Gfl1PfKFpN7/Yee2vmt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankumawat0/Next-Word-Prediction/blob/main/next_word_prediction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HXYTR141qoua"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "data = \"hello world hello machine hello learning hello AI My name karan kumawat\"\n",
        "\n",
        "# Tokenization\n",
        "words = list(set(data.split()))\n",
        "word_to_idx = {w: i for i, w in enumerate(words)}\n",
        "idx_to_word = {i: w for i, w in enumerate(words)}\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(words)\n",
        "hidden_size = 16   # Number of hidden units\n",
        "sequence_length = 2  # Length of input sequence\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Preparing training data\n",
        "def create_sequences(data, seq_length):\n",
        "    tokens = data.split()\n",
        "    X, Y = [], []\n",
        "    for i in range(len(tokens) - seq_length):\n",
        "        X.append([word_to_idx[word] for word in tokens[i:i+seq_length]])\n",
        "        Y.append(word_to_idx[tokens[i+seq_length]])\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "X, Y = create_sequences(data, sequence_length)\n"
      ],
      "metadata": {
        "id": "OWDiZNJbqvwq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # Input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # Hidden to output\n",
        "\n",
        "# Biases\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((vocab_size, 1))\n"
      ],
      "metadata": {
        "id": "GS8iSLDDq5Rr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))\n",
        "    return exp_x / np.sum(exp_x)\n"
      ],
      "metadata": {
        "id": "5K161T5Pq824"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the RNN\n",
        "for epoch in range(500):\n",
        "    total_loss = 0\n",
        "    for i in range(len(X)):\n",
        "        # Inputs and Targets\n",
        "        inputs = X[i]\n",
        "        target = Y[i]\n",
        "\n",
        "        # One-hot encoding for inputs\n",
        "        x_encoded = np.zeros((vocab_size, sequence_length))\n",
        "        for t, idx in enumerate(inputs):\n",
        "            x_encoded[idx, t] = 1\n",
        "\n",
        "        # Forward Pass\n",
        "        hs = np.zeros((hidden_size, sequence_length + 1))  # Hidden states\n",
        "        for t in range(sequence_length):\n",
        "            hs[:, t+1] = np.tanh(np.dot(Wxh, x_encoded[:, t]) + np.dot(Whh, hs[:, t]) + bh.flatten())\n",
        "\n",
        "        # Output layer\n",
        "        y_hat = softmax(np.dot(Why, hs[:, -1]) + by.flatten())  # Shape: (vocab_size,)\n",
        "\n",
        "        # Loss (Cross-Entropy)\n",
        "        loss = -np.log(y_hat[target])\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backpropagation\n",
        "        dy = y_hat\n",
        "        dy[target] -= 1  # Gradient of softmax + cross-entropy\n",
        "\n",
        "        # Reshape dy to (vocab_size, 1) for compatibility\n",
        "        dy = dy.reshape(-1, 1)\n",
        "\n",
        "        # Gradients for output layer\n",
        "        dWhy = np.dot(dy, hs[:, -1].reshape(1, -1))  # Shape: (vocab_size, hidden_size)\n",
        "        dby = dy                                    # Shape: (vocab_size, 1)\n",
        "\n",
        "        # Gradients for hidden layer\n",
        "        dh = np.dot(Why.T, dy)                      # Shape: (hidden_size, 1)\n",
        "        dWxh = np.zeros_like(Wxh)\n",
        "        dWhh = np.zeros_like(Whh)\n",
        "        dbh = np.zeros_like(bh)\n",
        "\n",
        "        for t in reversed(range(sequence_length)):\n",
        "            dtanh = (1 - hs[:, t+1] ** 2) * dh.flatten()   # Shape: (hidden_size,)\n",
        "\n",
        "            # Ensure dtanh is 1D for outer products\n",
        "            dtanh = dtanh.flatten()  # Shape: (hidden_size,)\n",
        "\n",
        "            # Gradients for Wxh, Whh, and bh\n",
        "            dWxh += np.outer(dtanh, x_encoded[:, t])       # Shape: (hidden_size, vocab_size)\n",
        "            dWhh += np.outer(dtanh, hs[:, t])              # Shape: (hidden_size, hidden_size)\n",
        "            dbh += dtanh.reshape(-1, 1)                    # Shape: (hidden_size, 1)\n",
        "\n",
        "            # Update dh for the next time step\n",
        "            dh = np.dot(Whh.T, dtanh.reshape(-1, 1))       # Shape: (hidden_size, 1)\n",
        "\n",
        "        # Gradient Descent Update\n",
        "        Wxh -= learning_rate * dWxh\n",
        "        Whh -= learning_rate * dWhh\n",
        "        Why -= learning_rate * dWhy\n",
        "        bh  -= learning_rate * dbh\n",
        "        by  -= learning_rate * dby  # Shapes match (vocab_size, 1)\n",
        "\n",
        "    # Monitoring the Loss\n",
        "    if epoch % 50 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {total_loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgcJjY02q_4P",
        "outputId": "b1e818f9-70e5-40c3-b436-dc72eda18586"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 21.9946\n",
            "Epoch 50, Loss: 20.6020\n",
            "Epoch 100, Loss: 20.1537\n",
            "Epoch 150, Loss: 19.3572\n",
            "Epoch 200, Loss: 15.6377\n",
            "Epoch 250, Loss: 12.3195\n",
            "Epoch 300, Loss: 9.7868\n",
            "Epoch 350, Loss: 7.7196\n",
            "Epoch 400, Loss: 5.9411\n",
            "Epoch 450, Loss: 4.7631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(sequence):\n",
        "    tokens = sequence.split()\n",
        "    if len(tokens) != sequence_length:\n",
        "        print(f\"Please enter a {sequence_length}-word sequence.\")\n",
        "        return\n",
        "\n",
        "    # Encoding the input sequence\n",
        "    x_encoded = np.zeros((vocab_size, sequence_length))\n",
        "    for t, word in enumerate(tokens):\n",
        "        if word not in word_to_idx:\n",
        "            print(f\"Word '{word}' not in vocabulary.\")\n",
        "            return\n",
        "        x_encoded[word_to_idx[word], t] = 1\n",
        "\n",
        "    # Forward Pass\n",
        "    hs = np.zeros((hidden_size, sequence_length + 1))\n",
        "    for t in range(sequence_length):\n",
        "        hs[:, t+1] = np.tanh(np.dot(Wxh, x_encoded[:, t]) + np.dot(Whh, hs[:, t]) + bh.flatten())\n",
        "\n",
        "    # Prediction\n",
        "    y_hat = softmax(np.dot(Why, hs[:, -1]) + by.flatten())\n",
        "    predicted_idx = np.argmax(y_hat)\n",
        "\n",
        "    return idx_to_word[predicted_idx]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0RFwN4oXrC_O"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Prediction\n",
        "while True:\n",
        "    # Taking input from the user\n",
        "    user_input = input(f\"Enter a {sequence_length}-word sequence (or type 'exit' to quit): \")\n",
        "\n",
        "    # Exit condition\n",
        "    if user_input.lower() == 'exit':\n",
        "        print(\"Exiting the program. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Predict the next word\n",
        "    print(predict_next_word(user_input))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45JHERkZvLmh",
        "outputId": "b533edcf-92f7-43df-8115-7cdd2a989be8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a 2-word sequence (or type 'exit' to quit): My name\n",
            "karan\n",
            "Enter a 2-word sequence (or type 'exit' to quit): hello world\n",
            "hello\n",
            "Enter a 2-word sequence (or type 'exit' to quit): exit\n",
            "Exiting the program. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eMEs0t1azLFy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}